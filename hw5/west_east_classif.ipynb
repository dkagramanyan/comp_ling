{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:23:38.928931Z",
     "end_time": "2023-05-29T19:23:38.952894Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from lmfit.models import Model\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pyconll\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('hy_wiki_10%.txt','w',encoding='utf-8') as f:\n",
    "    file=open('eastern/dump_wiki_hy.txt','r',encoding='utf-8').readlines()\n",
    "    normed_txt, x_test= train_test_split(file, test_size=0.9, random_state=42)\n",
    "    f.writelines(normed_txt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T17:03:53.739476Z",
     "end_time": "2023-05-29T17:04:00.739842Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UD to txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_conll_file_location = 'hy_bsut-ud-train.conllu'\n",
    "train = pyconll.load_from_file(my_conll_file_location)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('ud_eastern.txt', 'w') as f:\n",
    "    for sentence in train:\n",
    "        # Do work within loops\n",
    "        f.write(sentence.text + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stop-words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # punctuation=['.','-',',','!','?','(','—',')','՞','՛','։','՝','՜','’','«','»','*','\\n','=',':','[',']','/',';','․','`','\\t','%','$','\\xa0','\\r','_','●','0','1','2','3','4','5','6','7','8','9']\n",
    "    punctuation = ['՜', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '-', '—', '։']\n",
    "\n",
    "    for spaced in punctuation:\n",
    "        sentence = sentence.replace(spaced, '').lower()\n",
    "\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "\n",
    "    txt = sentence.replace('\\n', '').lower()\n",
    "    txt = txt.split(' ')\n",
    "    txt = [t for t in txt if t != '']\n",
    "    return txt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T17:04:20.905753Z",
     "end_time": "2023-05-29T17:04:20.916694Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "folders = ['eastern', 'western', 'grabar']\n",
    "worlds_list_dict = {}\n",
    "\n",
    "for folder in folders:\n",
    "    files_paths = glob.glob(folder + '/*.txt')\n",
    "    names = [path.replace('/', ' ')[:15] for path in files_paths]\n",
    "    files = [' '.join(open(path, 'r', encoding='utf8').readlines()) for path in files_paths]\n",
    "\n",
    "    words_list = []\n",
    "    for sentence in files:\n",
    "        words_list.extend(preprocess(sentence))\n",
    "\n",
    "    print(words_list.__len__())\n",
    "    worlds_list_dict[folder] = Counter(words_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T17:04:55.278662Z",
     "end_time": "2023-05-29T17:05:13.209268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('stop_eastern.txt', 'w') as f:\n",
    "    for i, line in enumerate(worlds_list_dict['eastern'].most_common()):\n",
    "        if line[1] >= 150:\n",
    "            f.write(str(line[0]) + '\\n')\n",
    "        else:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and split data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "folders = ['eastern', 'western', 'grabar']\n",
    "files_dict = {}\n",
    "\n",
    "for folder in folders:\n",
    "    files_paths = glob.glob(folder + '/*.txt')\n",
    "    files = [' '.join(open(path, 'r', encoding='utf8').readlines()) for path in files_paths]\n",
    "    files_dict[folder] = files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:10:57.298175Z",
     "end_time": "2023-05-29T18:10:58.410693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "files[0].__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T16:59:04.829990Z",
     "end_time": "2023-05-29T16:59:04.884829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocess_files_dict = {}\n",
    "batch_len = 30\n",
    "\n",
    "folders = ['eastern', 'western', 'grabar']\n",
    "tokens_number = np.array([8286090, 4648167, 785605])\n",
    "\n",
    "tokens_fraction=785606/tokens_number\n",
    "\n",
    "for i,folder in enumerate(folders):\n",
    "    files = files_dict[folder]\n",
    "    batches_list = []\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        words = preprocess(file)\n",
    "        batches = np.array_split(words, np.ceil(words.__len__() / batch_len))\n",
    "        if i!=2:\n",
    "            normed_batches, x_test= train_test_split(batches, test_size=1-tokens_fraction[i], random_state=42)\n",
    "        else:\n",
    "            normed_batches=batches\n",
    "        batches_list.extend(normed_batches)\n",
    "\n",
    "    preprocess_files_dict[folder] = batches_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:15.786517Z",
     "end_time": "2023-05-29T18:57:39.002949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_text = []\n",
    "dataset_labels = []\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    text_batches = preprocess_files_dict[folder]\n",
    "    dataset_text.extend(text_batches)\n",
    "    print(len(text_batches))\n",
    "    dataset_labels.extend(np.full(len(text_batches), i))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:39.003946Z",
     "end_time": "2023-05-29T18:57:39.043839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'text':dataset_text, 'labels':dataset_labels})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:39.052816Z",
     "end_time": "2023-05-29T18:57:39.098693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('mwa_mea_grabar_30.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:39.068773Z",
     "end_time": "2023-05-29T18:57:45.716675Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lexical (stop-words) descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "western_stop = ['ենք', 'էի', 'թ', 'ին', 'մենք', 'որոնք', 'պիտի', 'և', 'որպեսզի', 'վրայ', 'կ՚', 'կը', 'մը', 'մըն',\n",
    "                'անոր', 'ալ', 'ան', 'քեզ', 'եթէ', 'թէ', 'որպէս']\n",
    "\n",
    "grabar_stop = ['դու', 'եք', 'ըստ', 'նա', 'պիտի', 'վրայ', 'զի', 'ընդ', 'քո', 'քեզ', 'եթէ', 'թէ', 'որպէս']\n",
    "\n",
    "eastern_stop = ['դու', 'ենք', 'եք', 'էի', 'ըստ', 'ին', 'հետո', 'մենք', 'մեջ', 'նա', 'նաև', 'նրա', 'նրանք', 'որը',\n",
    "                'որոնք', 'որպես', 'ում', 'վրա', 'և', 'որպեսզի']\n",
    "\n",
    "western_stop = set(western_stop)\n",
    "grabar_stop = set(grabar_stop)\n",
    "eastern_stop = set(eastern_stop)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:45.719667Z",
     "end_time": "2023-05-29T18:57:45.733013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lexical_desc(words):\n",
    "    intersect_western = len(set(words) & western_stop) / len(western_stop)\n",
    "    intersect_grabar = len(set(words) & grabar_stop) / len(grabar_stop)\n",
    "    intersect_eastern = len(set(words) & eastern_stop) // len(eastern_stop)\n",
    "\n",
    "    return intersect_western, intersect_grabar, intersect_eastern"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:45.735008Z",
     "end_time": "2023-05-29T18:57:45.761972Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Morphemic descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grabar_suffixes = ['աւք', 'եալ', 'եան', 'իւք', 'ոյց', 'ովք', 'ուց', 'ուցան']\n",
    "grabar_prefixes = ['ապա', 'արտ', 'բաղ', 'բաղա', 'դեր', 'ենթ', 'ենթա', 'ընթա', ' համ', 'համա', 'հան', 'հոմ', 'հոմա',\n",
    "                   'տար', 'տարա']\n",
    "\n",
    "eastern_suffixes = ['աբար', 'ագին', 'ագույն', 'ածո', 'ածու', 'ական', 'ակերտ', 'ային', 'անակ', 'անի', 'անոց', 'անք',\n",
    "                    'ապան', 'ապանակ', 'ապատ', 'ապես', 'աստան', 'ավետ', 'ավուն', 'արան', 'արար', 'արեն', 'արք', 'ացի',\n",
    "                    'ացն-', 'ացու', 'բան', 'բար', 'գին', 'գույն', 'եղեն', 'ենի', 'երեն', 'երորդ', 'եցն-', 'լիկ', 'կերտ',\n",
    "                    'կոտ', 'մունք ', 'յալ', 'յակ', 'յան', 'յանց', 'յուն նախա-', 'ներ', 'նոց', 'ոնք', 'ովին', 'որդ',\n",
    "                    'որեն', 'ոցի', 'ուք', 'պան', 'պանակ', 'ստան', 'ված', 'վածք', 'ավոր', 'վոր', 'ություն', 'ուլ', 'ուկ',\n",
    "                    'ուհի', 'ում', 'ույթ', 'ույր', 'ուն', 'ուտ', 'ուրդ', 'ուց']\n",
    "eastern_prefixes = ['ամենա', 'այսր', 'անդր', 'ապա', 'ավտո', 'արտ', 'արտա', 'բենզա', ', գեր', 'գերա', 'դեր', 'ենթա',\n",
    "                    'եվրա', ' էլեկտրա', 'թեր', 'թերա', 'կենս', 'կինո', 'հակ', 'հակա', 'համ', 'համա', 'հար', 'հարա',\n",
    "                    'հեռա', 'հեռուստա', 'հոմա', 'մակ', 'մակրո', 'միկրո', 'միջ', 'նախ', 'ներ', 'ստոր', 'վեր', 'վերա',\n",
    "                    'տար', 'տարա', 'փոխ', 'քառ', 'քառա']\n",
    "\n",
    "western_reform = ['իլ', 'իուն', 'եան', 'յ', 'օ', 'է', 'յ', 'վո', 'ոյ', 'եա', 'եօ', 'իւ', 'ու', 'ւ,' 'յե', 'եյ', 'զի',\n",
    "                  'եւ', 'ել', 'յուն', 'յան', 'ում', 'ո', 'ե', 'հ', 'ո', 'ույ', 'յա', 'յո', 'յու', 'վ', 'ե', ]\n",
    "\n",
    "morphems=[]\n",
    "morphems.extend(grabar_suffixes)\n",
    "morphems.extend(grabar_prefixes)\n",
    "morphems.extend(eastern_suffixes)\n",
    "morphems.extend(eastern_prefixes)\n",
    "morphems.extend(western_reform)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T18:57:45.754961Z",
     "end_time": "2023-05-29T18:57:45.776929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_morphemic_desc(words, morphema):\n",
    "\n",
    "    positions = []\n",
    "    for word in words:\n",
    "        pos = word.find(morphema)\n",
    "        if pos != -1:\n",
    "            positions.append((pos+1)/len(word))\n",
    "\n",
    "    if positions.__len__()==0:\n",
    "        positions=[0,0,0]\n",
    "\n",
    "    std=np.std(positions)\n",
    "    # mean=np.mean(positions)\n",
    "\n",
    "    return std\n",
    "    # return mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:09:26.829842Z",
     "end_time": "2023-05-29T19:09:26.845799Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_desc_lexical=[get_lexical_desc(text) for text in tqdm(dataset_text)]\n",
    "\n",
    "dataset_desc_morphemic=[]\n",
    "for i,morph in enumerate(tqdm(morphems)):\n",
    "    dataset_desc_morphemic.append([])\n",
    "    dataset_desc_morphemic[i]=[get_morphemic_desc(text,morph) for text in dataset_text]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:10:20.128610Z",
     "end_time": "2023-05-29T19:21:35.793387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_desc_morphemic=np.array(dataset_desc_morphemic)\n",
    "dataset_desc_lexical=np.array(dataset_desc_lexical)\n",
    "dataset_desc_morphemic=np.swapaxes(dataset_desc_morphemic,0,1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:36.369481Z",
     "end_time": "2023-05-29T19:21:36.383443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset=np.concatenate([dataset_desc_morphemic,dataset_desc_lexical],axis=1)\n",
    "dataset.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:36.385438Z",
     "end_time": "2023-05-29T19:21:36.429320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset, dataset_labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:36.431315Z",
     "end_time": "2023-05-29T19:21:36.494147Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create RTF model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=15, random_state=51,verbose=1)\n",
    "clf.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:36.495145Z",
     "end_time": "2023-05-29T19:21:44.752767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.score(x_test,y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:44.754761Z",
     "end_time": "2023-05-29T19:21:44.985902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted=clf.predict(x_test)\n",
    "conf_mat = confusion_matrix(y_test, predicted)\n",
    "print(conf_mat/sum(conf_mat))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:44.972937Z",
     "end_time": "2023-05-29T19:21:45.190625Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-29T19:21:59.381216Z",
     "end_time": "2023-05-29T19:21:59.414127Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
